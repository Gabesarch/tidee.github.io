
<!DOCTYPE html>
<html>
  <head>
    <title>TIDEE: Novel Room Reorganization using Visuo-Semantic Common Sense Priors</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <!-- <link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'> -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="css/style.css">
    <style>
      html,body,h1,h2,h3,h4,h5,h6 {font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;}
      <!-- .cite { background:#f0f0f0; padding:10px; font-size:18px} -->
      .cite { padding:0px; background:#ffffff; font-size:18px}
      .card {border: 1px solid #ccc}
      img { margin-bottom:-6px;}
      p { font-size:18px;}
      a {text-decoration: none; color: #2196F3;}
      .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
      0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
      5px 5px 0 0px #fff, /* The second layer */
      5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
      10px 10px 0 0px #fff, /* The third layer */
      10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
      15px 15px 0 0px #fff, /* The fourth layer */
      15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
      20px 20px 0 0px #fff, /* The fifth layer */
      20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
      25px 25px 0 0px #fff, /* The fifth layer */
      25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 60px;
      }
    </style>
  </head>  
  <body class="w3-white">
    <!-- Page Container -->
    <div class="w3-content w3-margin-top w3-margin-bottom" style="max-width:960px;">

      <!-- The Grid -->
      <div class="w3-row-padding">

	<!-- paper container -->	  
	<div class="w3-display-container w3-row w3-white w3-margin-bottom">
	  <div class="w3-center">
	    <h1>TIDEE: Novel Room Reorganization using Visuo-Semantic Common Sense Priors</h1>
	    <h5><a href="https://www.gabesarch.me">Gabriel Sarch</a> &emsp;&emsp; <a href="https://zfang399.github.io/">Zhaoyuan Fang</a> &emsp;&emsp; <a href="https://cs.cmu.edu/~aharley/">Adam Harley</a> &emsp;&emsp; <a href="https://paulschydlo.com/">Paul Schydlo</a></h5>
        <h5><a href="https://www.cs.cmu.edu/~katef/">Michael Tarr</a> &emsp;&emsp; <a href="https://www.cs.cmu.edu/~katef/">Saurabh Gupta</a> &emsp;&emsp; <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></h5>
	    <!--<h5><em>CVPR 2021</em></h5>-->
	  </div>
	  <!--
	  <div class="w3-center">
	    <h3>[<a href="https://github.com/aharley/track_check_repeat">Code</a>] &emsp; [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Harley_Track_Check_Repeat_An_EM_Approach_to_Unsupervised_Tracking_CVPR_2021_paper.pdf">Paper</a>]</h3>
	  </div>
	  -->
      <div class="column has-text-centered">
        <div class="publication-links">
          <!-- PDF Link. -->
          <span class="link-block">
            <a href="https://arxiv.org/pdf/2011.12948"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="fas fa-file-pdf"></i>
              </span>
              <span>Paper</span>
            </a>
          </span>
          <span class="link-block">
            <a href="https://arxiv.org/abs/2011.12948"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
          </span>
          <!-- Video Link. -->
          <span class="link-block">
            <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="fab fa-youtube"></i>
              </span>
              <span>Video</span>
            </a>
          </span>
          <!-- Code Link. -->
          <span class="link-block">
            <a href="https://github.com/google/nerfies"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
              </a>
          </span>
          <!-- Dataset Link. -->
          <span class="link-block">
            <a href="https://github.com/google/nerfies/releases/tag/0.1"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="far fa-images"></i>
              </span>
              <span>Data</span>
              </a>
        </div>
      </div>

	  <hr>
	  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
	    <img src="images/fig1.png" style="width:100%">
	  </div>
	  <hr>
	  <div class="w3-center">
	    <h2>Abstract</h2>
	  </div>
	  <p align="justify">We introduce TIDEE, an embodied agent that tides up a disordered scene based on learned commonsense object placement and room arrangement priors. TIDEE explores a home environment, detects objects that are out of their natural place, infers plausible object contexts for them, localizes such contexts in the current scene, and repositions the objects. Commonsense priors are encoded in three modules: i) visuo-semantic detectors that detect out of place objects, ii) an associative neural graph memory of objects and spatial relations that proposes plausible semantic receptacles for object repositions, and iii) a visual search network that guides the agentâ€™s exploration for efficiently localizing the receptacle-of-interest in the current scene to reposition the object. We test TIDEE on tidying up disorganized scenes in the AI2THOR simulation environment. TIDEE carries out the task without ever having observed the same room beforehand, relying only on priors learned from a separate set of training houses. We evaluate performance by scoring the resulting room reorganizations using human evaluators. TIDEE out performs ablative versions of the model that do not use one or more of the commonsense priors. Moreover, we show TIDEE can successfully be instructed in natural language to follow specifications for object placement, without any additional training data. Last, on the related room rearrangement benchmark [3], TIDEE outperforms the state-of-the-art by a 7x performance margin.</p>
	  <hr>

	  <!--
	  <div class="w3-center">
	    <h2>Video</h2>
	  </div>
	  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
	    <iframe width="800" height="600" src="https://www.youtube.com/embed/Jg2f5fkgxZo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	  </div>
	  <hr>
	  -->

	  <div class="w3-center">
	    <h2>Demo</h2>
	  </div>

      <div class="w3-center">
	    <h2>Video</h2>
	  </div>

      <section class="section" id="overflow-example">
        <div>
          <h2 class="title">BibTeX</h2>
        </div>
          <pre style="background-color:#F5F5F5;"><code style="color:black">@article{park2021nerfies,
        author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
        title     = {Nerfies: Deformable Neural Radiance Fields},
        journal   = {ICCV},
        year      = {2021},
      }</code></pre>
      <div>
    </div>
      </section>
	  
	  <!-- <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center"> -->
	  <!--   <iframe width="800" height="600" src="https://www.youtube.com/embed/Jg2f5fkgxZo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
	  <!-- </div> -->
	  <hr>
	  

	  <!-- <div class="w3-row w3-margin" style="padding-bottom:2em">
	    <div class="w3-center"><h2>Paper</h2></div>
	    <div class="w3-col s0 m1 l2" style="height:10px"></div>
	    <div class="w3-col s6 m3 l2">
	      <a href="https://arxiv.org/abs/2204.04153"><img class="layered-paper-big" src="images/page1.png" style="width:100%;min-height:200px; margin-right:3em"></a>
	    </div>
	    <div class="w3-col s6 m7 l6" style="padding-left:5em">
	      <div class="cite">
		Adam W. Harley, Zhaoyuan Fang, and Katerina Fragkiadaki.
		<i>Particle Videos Revisited: Tracking Through Occlusions Using Point Trajectories.</i> 
		arXiv 2022.
	      </div>
	      <h3><a href="https://arxiv.org/pdf/2204.04153.pdf">[pdf]</a>&emsp;<a href="bib.txt">[bibtex]</a></h3>
	    </div>
	    <div class="w3-col s0 m1 l2" style="height:10px"></div>
	  </div> -->
	  <hr>
	  
	  <!-- end paper container -->

	</div><!-- End Grid -->
      </div><!-- End Page Container -->

  </body>
</html>
